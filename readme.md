# Computer Vision CS-867 (Assignment #3)

Semantic Segmentation

**Muhammad Talha Imran MSDS-2022**

## Table of Contents

1. [Abstract](#Abstract)
2. [Introduction](#Introduction)
3. [Dataset and Augmentation](#Dataset)
    1. [Data Distribution](#DataDistribution)
4. [File Navigation](#Navigation)
    1. [Nomenclature](#Nomenclature)
5. [Implementation details](#Implementationdetails)
    1. [Usage instructions](#Usageinstructions)
    2. [Installing dependencies](#Installingdependencies)
    3. [Training Parameters](#TrainingParameters)
    4. [Training Visualizations](#TrainingVisualizations)
    5. [Data Logging](#logging)
6. [Results](#Results)
    1. [Quantitative Results](#QuantitativeResults)
    2. [Qualitative Results](#QualitativeResults)
7. [Detailed Assignment report](#DetailedAssignmentreport)
8. [Credits](#Credits)

<a name="Abstract"></a>

## Abstract

Extracting semantics from images or high-dimensional unstructured data is of great significance for many applications
such as autonomous driving, medical imaging, remote sensing, object detection, etc. The process of labeling each pixel
with its respective class is known as semantic segmentation. It is of great significance in the development of
autonomous systems, medical imaging, and remote sensing that a computer be able to understand the meaning of
unstructured data, just as humans are able to perceive the world around them and understand the meaning of images.
Segmentation of images is one stepping stone towards developing machines that can understand and perceive the
environment and world around them.

**Results from Unet++ using the resnet101 backbone**

![U_net_PP_resnet101_video.gif](Results%2FVideos%2Fgif_files%2FU_net_PP_resnet101_video.gif)

<a name="Introduction"/></a>

## Introduction

Semantic segmentation is the process of labeling each pixel with respect to its category/class. This has many
applications, such as autonomous driving, remote sensing, medical imaging, etc. The labels are assigned in the form of a
label map that is HxW, where H is the height of the image and W is the width. Several architectures have been proposed
over the years to handle such tasks. These models typically employ an encoder-decoder architecture, where the encoder is
responsible for feature extraction, i.e., creating a feature-dense representation of the original input image while
simultaneously downsampling the image. The decoder subsequently upsamples these feature representations while, in some
architectures, performing convolution operations as well. The samples generated by the decoder generates a feature map
that is the size of the original image. This feature map is then passed through a softmax layer to generate the
probability of each class

![Assignment_3_Illustrations.png](Accumulated_Data%2FAssignment_3_Illustrations.png)

Semantic segmentation does not discriminate between multiple instances of the same class occurring in a neighborhood,
such that they are not separated by another object.

![Assignment_3_Illustrations1.png](Accumulated_Data%2FAssignment_3_Illustrations1.png)

<a name="Dataset"></a>

## Dataset and Augmentation:

The dataset is comparatively small, while simultaneously, the labeling seems to be quite coarse in nature. The training
data was split into a training and validation set with the ratios 80-20, respectively. Due to the fewer number of data
samples available, a number of augmentation techniques were used to improve training performance. Provided below is a
list of the augmentations used and the rationale behind their use.

- **Horizontal flip:** It is quite common to encounter reflections of the same object, and most objects that exist in
  nature
  tend to have a very similar appearance compared to the original object.

- **Vertical flip:** Although uncommon in nature, it is possible that upside-down objects may be present in an
  environment
  around a sensing system.
- **Random brightness:** It is very common to see random brightness variations in nature with regards to changing light
  levels
  during the day. It is also possible to encounter random brightness fluctuations due to photometric anomalies presented
  by reflections, cloud covers, street lights, etc.

- **Gaussian blur:** It may be possible that the object is out of the plane of focus from the camera and may appear
  slightly
  blurred.

- **Cutouts:** Occlusions are a quite frequent occurrence in nature and in everyday life where the object of interest
  may be
  occluded by another object.

- **Gaussian noise:** This type of noise may be introduced due to variations in the cameraâ€™s sensitivity such as ISO
  variations, etc.

- **Resize image:** Images are resized to 256x256 since the used network requires the dimensions of the input image to
  be
  multiples of 16, and this is the closest downsample possible

**Dataset available [here](https://drive.google.com/drive/folders/1CWvhf_CtSTP1k8fFgG54cMV0GPMtqccm?usp=share_link)**

<a name="DataDistribution"></a>

#### Data Distribution:

| Dataset    | Samples |
|------------|---------|
| Train      | 293     |
| Validation | 74      |
| Test       | 101     |

<a name="Navigation"></a>

## File Navigation:

Directory System:

![File_navigation.PNG](Accumulated_Data%2FFile_navigation.PNG)

- **Accumulated_Data** >> Contains Visualizations used for readme.
- **Dataset** >> Contains the dataset(Not uploaded,
  dataset [link](https://drive.google.com/drive/folders/1CWvhf_CtSTP1k8fFgG54cMV0GPMtqccm?usp=share_link))
- **Models** >> Contains the trained models(**Trained Models are
  available [here](https://drive.google.com/drive/folders/1h0f-khok7UQCqT1ZDvyH-5Bc9p08joz6?usp=share_link)**. Linked
  separately due to large file size.)
- **Results** >> Contains the Results from training, validation and test.
- **SCR** >> Contains the Results from training

Dataset folder structure:

![Dataset_folder.PNG](Accumulated_Data%2FDataset_folder.PNG)

Results folder structure:

![Results_folder.PNG](Accumulated_Data%2FResults_folder.PNG)

<a name="Nomenclature"></a>

### Nomenclature:

**SCR folder**

- **Dataset.py** >> Contains the dataset class
- **HelperFunctions.py** >> Contains the Helper functions for training
- **test.py** >> Experimentation playground
- **U_net.py** >> Contains the Unet model script
- **U_net_PP.py** >> Contains the Unet++ model script
- **DeepLab_V3Plus.py** >> Contains the DeepLab_V3Plus model script
- **PANet.py** >> Contains the PANet model script

**Results folder**

- **Video_frames** >> Contains predict results on the dataset to generate video
- **Videos** >> Contains videos and gif files for readme and as examples for the reader.
- **{model_name}_{backbone}_experiment_data.csv** >> Contains training parameter for **{model_name}_{Backbone}**
- **{model_name}_{backbone}_quantitative_results.csv** >> Contains quantitative results for **{model_name}_{Backbone}**
- **{model_name}_{backbone}_Metrics.png** >> Contains training metrics for **{model_name}_{Backbone}**
- **{model_name}_{backbone}_qualitative_results.jpg** >> Contains qualitative results for **{model_name}_{Backbone}**
- **{model_name}_{backbone}_Training_Metrics.png** >> Contains training metrics for **{model_name}_{Backbone}**
- **{model_name}_{backbone}_Validation_Metrics.png** >> Contains Validation metrics for **{model_name}_{Backbone}**

**Models folder**

- **{model_name}_{backbone}__best_dice.pth** >> **{model_name}_{Backbone}** with best dice score during training
- **{model_name}_{backbone}__check_points.pth** >> **{model_name}_{Backbone}** as checkpoint during training even if
  dice score does not increase

<a name="Implementationdetails"></a>

## Implementation details:

<a name="Usageinstructions"></a>

### Usage instructions:

After downloading the models and dataset to their respective directory within the project directory.

use the following controls:
![Program_Controls.PNG](Accumulated_Data%2FProgram_Controls.PNG)

- **Run_training** >> **Boolean**, to execute training on the provided dataset, must be true even if resuming training,
  if you
  wish to get inference from trained model then set to False and the program will skip training
- **Resume_training** >> **Boolean**, to resume interrupted training set to True, else False. False will result in the
  model
  training from scratch if run_training is set to True. Presently, the auto resume functionality is not implemented and
  the user will have to enter the starting epoch manually(i.e. start from the ith epoch): Example use case is presented
  below. Warning: To start from scratch and run the complete desired number of epochs, set starting_epoc to zero **(not
  used in this implementation, leftover due to code reusage from Assignment2)**
- **load_model** >> **Boolean**, to load previously saved model from the Models directory.
- **run_test_set** >> **Boolean**, to run inference on the provided testset and generate performance metrics, saved to
  the
  Results directory
- **generate_video_frames** >> **Boolean**,generates video frames

- **generate_video** >> **Boolean**,creates a video from the frames saved

Set desired hyperparameters:

![hyper_parameters.PNG](Accumulated_Data%2Fhyper_parameters.PNG)

- backbone >> Desired encoder backbone
- model_name >> Name of the .py file running to standardize the names of the saved files and ease of later use
- batch_size >> sets the batch size param for dataloader
- learning_rate >> sets the starting learning rate for cosine annealing
- pretrained >> Boolean, This option is not valid for Custom model(proposed model),who's no pretrained weights exist on
  the ImageNet Dataset **(not used in this implementation, leftover due to code reusage from Assignment2)**
- epochs >> Total Epochs to run, in case of resumed training, this is the ending epoch
- classes >> Used where number of classes needs to be referenced
- device_name >> torch.cuda.get_device_name(0) >> Name of the GPU used
- device >> Training device used
- starting_time>> logs the start time for duration calculation
- class_names >> used for reference while generating outputs

Set Dataset paths and augmentations:

![Paths_and_aug.PNG](Accumulated_Data%2FPaths_and_aug.PNG)

Select model if needed, [Documentation](https://smp.readthedocs.io/en/latest/) for models library and if loss, optimizer
and/or schedular needs to be changed.

![Model and backbone.PNG](Accumulated_Data%2FModel%20and%20backbone.PNG)

Training_function:

![Training_function.PNG](Accumulated_Data%2FTraining_function.PNG)

### NOTE: Resume interrupted training was not implemented in this case due to smaller dataset size and faster training times compared to Assignment 2.

<a name="Installingdependencies"></a>

### Installing dependencies:

```commandline
 pip install -r requirements.txt
```

#### ForManualInstallation:

```commandline

albumentations==1.3.0
matplotlib==3.7.1
numpy==1.24.1
opencv_python==4.7.0.72
opencv_python_headless==4.7.0.72
pandas==2.0.1
Pillow==9.3.0
Pillow==9.5.0
segmentation_models_pytorch==0.3.2
torch==2.0.1+cu117
torchvision==0.15.2+cu117
tqdm==4.65.0

```

<a name="TrainingParameters"></a>

### Training Parameters:

- **Optimizer** >> Adam
- **Schedular** >> Cosine Annealing(0.0001 to 0.00001 with 100 steps)
- **Loss Function** >> Cross Entorpy Loss + Dice Loss

Rest of the settings are mentioned below:

**Experimental Settings**

| Experiment Settings |            |                  |                     |                   |            |        |                  |                     |                     |                |                |
|---------------------|------------|------------------|---------------------|-------------------|------------|--------|------------------|---------------------|---------------------|----------------|----------------|
| Experiments         | Model      | Backbone         | Start Learning_rate | End Learning_rate | Batch_size | Epochs | Tranfer_learning | max_validation_dice | Classification_loss | Execution_time | Per Epoch Time |
| Baseline            | Unet       | VGG16BN          | 0.001               | 0.00001           | 8          | 100    | True             | 0.771               | 0.269               | 1,153.7        | 11.5           |
|                     | Unet       | RESNET18         | 0.001               | 0.00001           | 8          | 100    | True             | 0.731               | 0.286               | 740.8          | 7.4            |
|                     | Unet       | Efficient Net B4 | 0.001               | 0.00001           | 8          | 100    | True             | 0.766               | 0.240               | 1,256.3        | 12.6           |
| Experiments         | Unet++     | RESNET101        | 0.001               | 0.00001           | 8          | 100    | True             | 0.742               | 0.286               | 2,681.4        | 26.8           |
|                     | Unet++     | Efficient Net B4 | 0.001               | 0.00001           | 8          | 100    | True             | 0.784               | 0.270               | 1,560.0        | 15.6           |
|                     | Unet++     | RESNEXT50        | 0.001               | 0.00001           | 8          | 100    | True             | 0.772               | 0.262               | 2,567.4        | 25.7           |
|                     | DeepLabV3+ | RESNET101        | 0.001               | 0.00001           | 8          | 100    | True             | 0.722               | 0.306               | 1,183.4        | 11.8           |
|                     | DeepLabV3+ | Efficient Net B4 | 0.001               | 0.00001           | 8          | 100    | True             | 0.746               | 0.270               | 1,183.5        | 11.8           |
|                     | DeepLabV3+ | RESNEXT50        | 0.001               | 0.00001           | 8          | 100    | True             | 0.743               | 0.266               | 1,054.1        | 10.5           |
|                     | PANet      | RESNET101        | 0.001               | 0.00001           | 8          | 100    | True             | 0.725               | 0.279               | 1,158.9        | 11.6           |
|                     | PANet      | Efficient Net B4 | 0.001               | 0.00001           | 8          | 100    | True             | 0.737               | 0.272               | 1,181.8        | 11.8           |
|                     | PANet      | RESNEXT50        | 0.001               | 0.00001           | 8          | 100    | True             | 0.748               | 0.271               | 1,022.9        | 10.2           |

<a name="TrainingVisualizations"></a>

### Training Visualizations:

**Training Performance for the Unet++ with the Efficient_net_b4 backbone**

![U_net_PP_efficientnet-b4_Training_Metrics.png](Results%2FU_net_PP_efficientnet-b4_Training_Metrics.png)

**Validation Performance for the Unet++ with the Efficient_net_b4 backbone**

![U_net_PP_efficientnet-b4_Validation_Metrics.png](Results%2FU_net_PP_efficientnet-b4_Validation_Metrics.png)

<a name="logging"></a>

### Data Logging

Data Logging was done using [Wandb](https://wandb.ai)

Training Reports are
available [here](https://drive.google.com/drive/folders/1gZZT-aQrLS7wA_vsW-74y9nHbjkkaPfj?usp=share_link)

<a name="Results"></a>

## Results:

<a name="QuantitativeResults"></a>

#### Quantitative Results:

| Test Set    |            |                  |               |          |                     |           |            |       |        |           |                   |       |          |       |       |          |       |            |       |       |            |           |
|-------------|------------|------------------|---------------|----------|---------------------|-----------|------------|-------|--------|-----------|-------------------|-------|----------|-------|-------|----------|-------|------------|-------|-------|------------|-----------|
| Experiments | Model      | Backbone         | Combined loss | Accuracy | Classification loss | Dice_loss | Dice_score | mIoU  | Recall | Precision | F1 per class mean |       |          |       |       |          |       |            |       |       |            |           |
|             |            |                  |               |          |                     |           |            |       |        |           | Background        | Sky   | Building | Pole  | Road  | Pavement | Tree  | SignSymbol | Fence | Car   | Pedestrian | Bicyclist |
| Baseline    | Unet       | VGG16BN          | 0.6283        | 0.987    | 0.356               | 0.272     | 0.728      | 0.645 | 0.921  | 0.921     | 0.968             | 0.911 | 0.189    | 0.983 | 0.929 | 0.949    | 0.584 | 0.789      | 0.721 | 0.490 | 0.755      | 0.338     |
|             | Unet       | RESNET18         | 0.7216        | 0.985    | 0.400               | 0.321     | 0.679      | 0.590 | 0.907  | 0.907     | 0.967             | 0.902 | 0.196    | 0.979 | 0.912 | 0.942    | 0.432 | 0.711      | 0.617 | 0.416 | 0.618      | 0.354     |
|             | Unet       | Efficient Net B4 | 0.6881        | 0.986    | 0.397               | 0.291     | 0.709      | 0.625 | 0.917  | 0.917     | 0.968             | 0.904 | 0.192    | 0.983 | 0.930 | 0.947    | 0.431 | 0.777      | 0.767 | 0.442 | 0.731      | 0.316     |
| Experiments | Unet++     | RESNET101        | 0.7642        | 0.985    | 0.440               | 0.324     | 0.676      | 0.592 | 0.907  | 0.907     | 0.967             | 0.903 | 0.088    | 0.979 | 0.915 | 0.944    | 0.510 | 0.704      | 0.761 | 0.401 | 0.596      | 0.284     |
|             | Unet++     | Efficient Net B4 | 0.6784        | 0.986    | 0.387               | 0.291     | 0.709      | 0.626 | 0.918  | 0.918     | 0.969             | 0.908 | 0.154    | 0.984 | 0.931 | 0.944    | 0.494 | 0.755      | 0.760 | 0.436 | 0.724      | 0.347     |
|             | Unet++     | RESNEXT50        | 0.6984        | 0.986    | 0.411               | 0.288     | 0.712      | 0.624 | 0.914  | 0.914     | 0.968             | 0.911 | 0.167    | 0.981 | 0.923 | 0.932    | 0.506 | 0.630      | 0.709 | 0.535 | 0.770      | 0.356     |
|             | DeepLabV3+ | RESNET101        | 0.7198        | 0.985    | 0.397               | 0.323     | 0.677      | 0.597 | 0.910  | 0.910     | 0.966             | 0.908 | 0.142    | 0.979 | 0.909 | 0.945    | 0.445 | 0.748      | 0.756 | 0.373 | 0.542      | 0.356     |
|             | DeepLabV3+ | Efficient Net B4 | 0.6880        | 0.986    | 0.384               | 0.304     | 0.696      | 0.615 | 0.916  | 0.916     | 0.966             | 0.905 | 0.088    | 0.981 | 0.915 | 0.945    | 0.402 | 0.778      | 0.742 | 0.437 | 0.731      | 0.370     |
|             | DeepLabV3+ | RESNEXT50        | 0.6869        | 0.986    | 0.381               | 0.306     | 0.695      | 0.613 | 0.914  | 0.914     | 0.966             | 0.903 | 0.110    | 0.979 | 0.912 | 0.946    | 0.440 | 0.775      | 0.717 | 0.432 | 0.673      | 0.354     |
|             | PANet      | RESNET101        | 0.7193        | 0.986    | 0.405               | 0.315     | 0.685      | 0.603 | 0.913  | 0.913     | 0.966             | 0.905 | 0.134    | 0.979 | 0.915 | 0.945    | 0.425 | 0.753      | 0.705 | 0.381 | 0.661      | 0.368     |
|             | PANet      | Efficient Net B4 | 0.6834        | 0.986    | 0.371               | 0.312     | 0.688      | 0.608 | 0.915  | 0.915     | 0.966             | 0.907 | 0.151    | 0.981 | 0.920 | 0.948    | 0.443 | 0.770      | 0.687 | 0.428 | 0.668      | 0.331     |
|             | PANet      | RESNEXT50        | 0.7157        | 0.985    | 0.398               | 0.318     | 0.682      | 0.596 | 0.912  | 0.912     | 0.967             | 0.908 | 0.093    | 0.979 | 0.915 | 0.945    | 0.498 | 0.734      | 0.661 | 0.477 | 0.614      | 0.268     |

<a name="QualitativeResults"></a>

#### Qualitative Results:

**Few Mask prediction examples for the Unet++ with the Efficient_net_b4 backbone**

![U_net_PP_efficientnet-b4_qualitative_results.jpg](Results%2FU_net_PP_efficientnet-b4_qualitative_results.jpg)
<a name="DetailedAssignmentreport"></a>

## Detailed Assignment report:

**Available [here](https://drive.google.com/drive/folders/1KX51gnUIVgop-15Sze-z_UNINKxda7LG?usp=share_link)**
<a name="Credits"></a>

## Credits:

This implementation is based on the
**[Segmentation Models Pytorch Library](https://github.com/qubvel/segmentation_models.pytorch/tree/master)**


